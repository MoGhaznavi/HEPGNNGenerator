{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T20:38:56.198859Z",
     "start_time": "2025-11-18T20:38:50.101611Z"
    }
   },
   "cell_type": "code",
   "source": "from RunningMultipleModels_with_logging import *",
   "id": "dcbbc6a21839034a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T20:39:34.738844Z",
     "start_time": "2025-11-18T20:38:56.230986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_path = \"/storage/mxg1065/datafiles\"\n",
    "shared_data = load_shared_data(load_path)\n",
    "print(\" Shared data loaded successfully!\")"
   ],
   "id": "84ceecfa400dde3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shared data...\n",
      "Loaded data shapes:\n",
      "  scaled_data['data_0']: (187650, 3)\n",
      "  unscaled_data['data_0']: (187650, 3)\n",
      "  neighbor_pairs_list: (1250242, 2)\n",
      "  labels_for_neighbor_pairs: (1000, 1250242)\n",
      " Shared data loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T20:39:36.714990Z",
     "start_time": "2025-11-18T20:39:34.877106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "available_gpus = torch.cuda.device_count()\n",
    "print(f\"\\n Available GPUs: {available_gpus}\")\n",
    "for i in range(available_gpus):\n",
    "    print(f\"   GPU {i}: {torch.cuda.get_device_name(i)} - \",\n",
    "          f\"{torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n"
   ],
   "id": "5423f2163f91075a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Available GPUs: 4\n",
      "   GPU 0: NVIDIA A30 -  23.5 GB\n",
      "   GPU 1: NVIDIA A30 -  23.5 GB\n",
      "   GPU 2: NVIDIA A30 -  23.5 GB\n",
      "   GPU 3: NVIDIA A30 -  23.5 GB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T20:39:36.810310Z",
     "start_time": "2025-11-18T20:39:36.803382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Base hyperparameters and defaults shared by all GPU configs\n",
    "base_config = {\n",
    "    'num_features': 3,\n",
    "    'num_classes': 5,\n",
    "    'hidden_dim': 128,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 5e-4,\n",
    "    'epochs': 200,\n",
    "    'batch_size': 1,\n",
    "    'save_dir': \"/storage/afarbin/training/models\",\n",
    "    'resume': True,\n",
    "    'patience': 20,\n",
    "    'delta': 0.0001,\n",
    "    'debug': True,\n",
    "    'weighted': False,\n",
    "    'auto_convert_results': False,\n",
    "    'save_comprehensive_table': True,\n",
    "    'generator_flags': {  # defaults for the data generator\n",
    "        'is_bi_directional': True,\n",
    "        'train_ratio': 0.7\n",
    "    },\n",
    "    'model_flags': {     # defaults for the model\n",
    "        'num_layers': 6,\n",
    "        'layer_weights': False,\n",
    "        'softmax': False\n",
    "    }\n",
    "}\n"
   ],
   "id": "708f6438369d776b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T20:39:36.887486Z",
     "start_time": "2025-11-18T20:39:36.876567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define per-GPU configurations with small variations\n",
    "configs = [\n",
    "    # GPU 0\n",
    "    {**base_config,\n",
    "    'model_name': \"fixed_generator_bs2_model.pt\",\n",
    "    'description': \"Model with two events per batch\",\n",
    "    'batch_size': 2}\n",
    "    #,\n",
    "\n",
    "    # # GPU 1\n",
    "    # {**base_config,\n",
    "    #  'model_name': \"nine_layer_model.pt\",\n",
    "    #  'description': \"Model with nine layers\",\n",
    "    #  'model_flags': {**base_config['model_flags'], 'num_layers': 9}},\n",
    "\n",
    "    # # GPU 2\n",
    "    #{**base_config,\n",
    "    # 'model_name': \"twelve_layer_model.pt\",\n",
    "    # 'description': \"Model with twelve layers\",\n",
    "    # 'model_flags': {**base_config['model_flags'], 'num_layers': 12}},\n",
    "\n",
    "   # # GPU 3\n",
    "   # {**base_config,\n",
    "   #  'model_name': \"fifteen_layer_model.pt\",\n",
    "   #  'description': \"Model with fifteen layers\",\n",
    "   #  'model_flags': {**base_config['model_flags'], 'num_layers': 15}}\n",
    "]\n",
    "\n",
    "# Trim configs to match the number of detected GPUs\n",
    "configs = configs[:available_gpus]\n",
    "\n",
    "# Display planned training jobs for confirmation\n",
    "print(f\"\\n Configuring {len(configs)} models:\")\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"   GPU {i}: {config['description']}\")\n",
    "    print(f\"      → Model: {config['model_name']}\")\n",
    "    print(f\"      → Hidden dim: {config['hidden_dim']}, LR: {config['lr']}\")\n",
    "    print(f\"      → Weighted: {config['weighted']}\")\n",
    "    print(f\"      → Auto-convert: {config['auto_convert_results']}\")\n"
   ],
   "id": "1b5db349c208ea31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Configuring 1 models:\n",
      "   GPU 0: Model with two events per batch\n",
      "      → Model: fixed_generator_bs2_model.pt\n",
      "      → Hidden dim: 128, LR: 0.001\n",
      "      → Weighted: False\n",
      "      → Auto-convert: False\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-18T20:39:36.947695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpu_id=0\n",
    "print(f\"   Clearing cache for GPU {gpu_id}...\")\n",
    "with torch.cuda.device(gpu_id):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()  # ensure cleanup completes\n",
    "\n",
    "# Show available memory for that GPU\n",
    "    free_mem = torch.cuda.mem_get_info(gpu_id)[0] / 1024**3\n",
    "    print(f\"   GPU {gpu_id} free memory before start: {free_mem:.2f} GB\")\n",
    "\n",
    "    config = configs[0]\n",
    "    result = train_single_gpu_with_logging(gpu_id, config, shared_data)\n"
   ],
   "id": "2d69bf039c79f8f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Clearing cache for GPU 0...\n",
      "   GPU 0 free memory before start: 23.27 GB\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Summarize training results by reading each GPU's metrics file\n",
    "print(\"\\n Training Summary:\")\n",
    "for i, config in enumerate(configs):\n",
    "    model_path = os.path.join(config['save_dir'], config['model_name'])\n",
    "    metrics_path = os.path.splitext(model_path)[0] + \".pkl\"\n",
    "    if os.path.exists(metrics_path):\n",
    "        try:\n",
    "            with open(metrics_path, 'rb') as f:\n",
    "                metrics = pickle.load(f)\n",
    "            best_acc = metrics.get('best_test_acc', 0)\n",
    "            total_time = metrics.get('total_time', 0)\n",
    "            min_time, sec_time = divmod(total_time, 60)\n",
    "            print(f\"   GPU {i}: {config['description']}\")\n",
    "            print(f\"      Best Accuracy: {best_acc:.4f}\")\n",
    "            print(f\"      Total Time: {int(min_time)}m {sec_time:.1f}s\")\n",
    "            print(f\"      Model: {model_path}\")\n",
    "        except:\n",
    "            print(f\"   GPU {i}: Could not load metrics\")\n",
    "    else:\n",
    "        print(f\"   GPU {i}: No results found\")\n",
    "\n",
    "print(\"\\n Multi-GPU training completed successfully!\")\n",
    "\n"
   ],
   "id": "7fa6542fe6431d01",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
