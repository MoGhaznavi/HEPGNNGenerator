{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91e9376-7b37-4015-9a91-364159bb3d0f",
   "metadata": {},
   "source": [
    "# Comprehensive Particle Physics Analysis Notebook\n",
    "## Multi-Model Performance & Cluster Visualization\n",
    "**Files Used:**\n",
    "`comprehensive_query_table_{model_name}_model.parquet`\n",
    "`{model_name}_model_raw_pairs.parquet` \n",
    "`{model_name}_model_T_pairs.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae4dd1-e6a6-4bd7-9689-c2e8d4606250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting Style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Central definitions\n",
    "class_names = ['Lone-Lone', 'True-True', 'Cluster-Lone', 'Lone-Cluster', 'Cluster-Cluster']\n",
    "class_colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "DEFAULT_COLUMNS = [\n",
    "    'event_id', 'edge_id', 'source_id', 'target_id',\n",
    "    'true_label', 'pred_label', 'is_correct', 'confidence',\n",
    "    'confidence_class_0', 'confidence_class_1', 'confidence_class_2',\n",
    "    'confidence_class_3', 'confidence_class_4',\n",
    "    'snr_source', 'eta_source', 'phi_source',\n",
    "    'snr_target', 'eta_target', 'phi_target',\n",
    "    'delta_eta', 'delta_phi', 'spatial_distance',\n",
    "    'avg_snr', 'event_size'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983d7f1-9209-4237-bf7d-3ad566177d24",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad0079e-71cf-4b7d-b0d0-4da126370c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comprehensive_data(model_name: str, file_pwd: str,\n",
    "                            columns: Optional[List[str]] = None,\n",
    "                            use_polars: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the comprehensive query table for a specific model.\n",
    "    By default uses pandas + pyarrow and projects columns to reduce IO.\n",
    "    Set use_polars=True to use Polars (faster on large datasets if installed).\n",
    "    \"\"\"\n",
    "    cols = columns or DEFAULT_COLUMNS\n",
    "    file_path = f\"{file_pwd}/comprehensive_query_table_{model_name}_model.parquet\"\n",
    "    print(f\"-> Loading: {file_path}\")\n",
    "\n",
    "    if use_polars:\n",
    "        try:\n",
    "            df_pl = pl.read_parquet(file_path, columns=cols)\n",
    "            df = df_pl.to_pandas()  # convert to pandas for rest of pipeline\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\"Polars requested but not installed or failed to load file.\") from e\n",
    "    else:\n",
    "        # use pyarrow engine and column projection\n",
    "        df = pd.read_parquet(file_path, engine='pyarrow', columns=cols)\n",
    "\n",
    "    print(f\"Loaded {len(df):,} rows from {file_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_raw_pairs_data(model_name: str, file_pwd: str) -> pd.DataFrame:\n",
    "    file_path = f\"{file_pwd}/{model_name}_model_raw_pairs.parquet\"\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    print(f\"Loaded raw pairs: {len(df):,} rows\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_tensor_data(model_name: str, file_pwd: str) -> np.ndarray:\n",
    "    file_path = f\"{file_pwd}/{model_name}_model_T_pairs.npy\"\n",
    "    tensor = np.load(file_path, mmap_mode='r')  # memory-map for large files\n",
    "    print(f\"Loaded tensor: {tensor.shape}\")\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580f1bb-b6e1-4ac6-9d92-376fcf7f5400",
   "metadata": {},
   "source": [
    "## Downcasting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac95b0-257f-4606-a9bc-cce27b36c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df: pd.DataFrame, float_cols: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downcast commonly used columns to memory-efficient dtypes.\n",
    "    \"\"\"\n",
    "    # integer columns\n",
    "    for c in ['true_label', 'pred_label']:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype('int8')\n",
    "    if 'event_id' in df.columns:\n",
    "        try:\n",
    "            df['event_id'] = df['event_id'].astype('int32')\n",
    "        except Exception:\n",
    "            pass\n",
    "    if 'is_correct' in df.columns:\n",
    "        df['is_correct'] = df['is_correct'].astype('bool')\n",
    "\n",
    "    # float columns: convert confidence_class_* and confidence to float32\n",
    "    for c in df.columns:\n",
    "        if c.startswith('confidence_class_') or c in ('confidence', 'spatial_distance', 'avg_snr', 'snr_source', 'snr_target', 'eta_source', 'eta_target', 'phi_source', 'phi_target'):\n",
    "            df[c] = df[c].astype('float32')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770871fe-303a-4b6f-8c87-51fd23b00247",
   "metadata": {},
   "source": [
    "## Analysis Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9580d9-c2ea-49a9-9fe2-30db6caba7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_roc_data_from_df(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (y_true, y_score) as numpy arrays (float32)\"\"\"\n",
    "    # Guarantee columns exist\n",
    "    required_conf_cols = [f'confidence_class_{i}' for i in range(len(class_names))]\n",
    "    missing = [c for c in required_conf_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing confidence columns: {missing}\")\n",
    "\n",
    "    y_true = df['true_label'].to_numpy(dtype=np.int8)\n",
    "    # Stack confidence columns into a (N, num_classes) float32 array efficiently\n",
    "    y_score = np.vstack([df[c].to_numpy(dtype=np.float32) for c in required_conf_cols]).T\n",
    "    return y_true, y_score\n",
    "\n",
    "def compute_tt_threshold(y_true: np.ndarray, y_score: np.ndarray, tt_tpr_target: float = 0.99) -> float:\n",
    "    \"\"\"\n",
    "    Compute the threshold for True-True class to reach tt_tpr_target (if possible).\n",
    "    y_true: 1D array of integer class labels\n",
    "    y_score: (N, C) float array\n",
    "    \"\"\"\n",
    "    tt_idx = class_names.index(\"True-True\")\n",
    "    y_true_bin = label_binarize(y_true, classes=np.arange(len(class_names)))\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_bin[:, tt_idx], y_score[:, tt_idx])\n",
    "    if tpr.max() >= tt_tpr_target:\n",
    "        tt_threshold = thresholds[np.where(tpr >= tt_tpr_target)[0][0]]\n",
    "    else:\n",
    "        tt_threshold = thresholds[np.argmax(tpr)]\n",
    "    return float(tt_threshold)\n",
    "\n",
    "def apply_tt_only_threshold_fast(df: pd.DataFrame, y_true: np.ndarray, y_score: np.ndarray,\n",
    "                                 tt_tpr_target: float = 0.99) -> Tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Vectorized: apply True-True threshold override only.\n",
    "    Returns modified df (copy) and the tt_threshold.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    tt_idx = class_names.index(\"True-True\")\n",
    "    tt_threshold = compute_tt_threshold(y_true, y_score, tt_tpr_target)\n",
    "    tt_scores = y_score[:, tt_idx]  # (N,)\n",
    "    tt_override_mask = tt_scores >= tt_threshold  # boolean mask (N,)\n",
    "\n",
    "    # Use index alignment to set values in df\n",
    "    idxs = df.index.to_numpy()\n",
    "    override_idxs = idxs[tt_override_mask]\n",
    "    # Apply vectorized assignment\n",
    "    if len(override_idxs) > 0:\n",
    "        df.loc[override_idxs, 'pred_label'] = np.int8(tt_idx)\n",
    "        df.loc[override_idxs, 'confidence'] = tt_scores[tt_override_mask].astype(np.float32)\n",
    "\n",
    "    # Recompute is_correct vectorized\n",
    "    df['is_correct'] = (df['pred_label'].to_numpy(dtype=np.int8) == df['true_label'].to_numpy(dtype=np.int8))\n",
    "    return df, tt_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c625a-e40e-433a-9b89-6881e18fb434",
   "metadata": {},
   "source": [
    "## Performance Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3c1ae-b65e-49e6-9da7-b08ba34e620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_indices(n: int, max_rows: int = 2_000_000, seed: int = 42) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return a list of indices to sample from a dataset of size n.\n",
    "    If n <= max_rows, returns None (no downsampling).\n",
    "    \"\"\"\n",
    "    if n <= max_rows:\n",
    "        return None\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.choice(n, size=max_rows, replace=False)\n",
    "\n",
    "def plot_loss_and_accuracy(metrics, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training/test loss and accuracy curves from your metrics dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = np.array(metrics.get(\"train_loss\", []))\n",
    "    test_loss  = np.array(metrics.get(\"test_loss\", []))\n",
    "    train_acc  = np.array(metrics.get(\"train_acc\", []))\n",
    "    test_acc   = np.array(metrics.get(\"test_acc\", []))\n",
    "\n",
    "    epochs = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axs[0].plot(epochs, train_loss, label=\"Train Loss\", linewidth=2)\n",
    "    axs[0].plot(epochs, test_loss, label=\"Test Loss\", linewidth=2)\n",
    "    axs[0].set_title(\"Loss Over Epochs\", fontsize=14)\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axs[1].plot(epochs, train_acc, label=\"Train Accuracy\", linewidth=2)\n",
    "    axs[1].plot(epochs, test_acc, label=\"Test Accuracy\", linewidth=2)\n",
    "    axs[1].set_title(\"Accuracy Over Epochs\", fontsize=14)\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_ylim(0, 1.0)\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "        print(f\"Saved loss/accuracy curves to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curves(y_true: np.ndarray, y_score: np.ndarray,\n",
    "                    sample_idx: Optional[np.ndarray] = None,\n",
    "                    title_suffix: str = \"\"):\n",
    "    \"\"\"Plot ROC for each class using provided arrays (fast).\"\"\"\n",
    "    if sample_idx is not None:\n",
    "        y_true = y_true[sample_idx]\n",
    "        y_score = y_score[sample_idx, :]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, cname in enumerate(class_names):\n",
    "        fpr, tpr, _ = roc_curve(label_binarize(y_true, classes=np.arange(len(class_names)))[:, i], y_score[:, i])\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"{cname} (AUC = {auc(fpr, tpr):.3f})\", color=class_colors[i])\n",
    "    plt.plot([0,1],[0,1],'k--',lw=2,label=\"Random classifier\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curves - Multi-class{title_suffix}\")\n",
    "    plt.legend(loc=\"lower right\"); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_precision_recall_curves(y_true: np.ndarray, y_score: np.ndarray,\n",
    "                                 sample_idx: Optional[np.ndarray] = None,\n",
    "                                 title_suffix: str = \"\"):\n",
    "    if sample_idx is not None:\n",
    "        y_true = y_true[sample_idx]\n",
    "        y_score = y_score[sample_idx, :]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, cname in enumerate(class_names):\n",
    "        precision, recall, _ = precision_recall_curve(label_binarize(y_true, classes=np.arange(len(class_names)))[:, i], y_score[:, i])\n",
    "        plt.plot(recall, precision, lw=2, label=f\"{cname} (AP = {auc(recall, precision):.3f})\", color=class_colors[i])\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curves{title_suffix}\")\n",
    "    plt.legend(loc=\"lower left\"); plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix_from_df(df: pd.DataFrame, title_suffix: str = \"\", normalize: bool = True, sample_idx: Optional[np.ndarray] = None):\n",
    "    if sample_idx is not None:\n",
    "        df_plot = df.iloc[sample_idx]\n",
    "    else:\n",
    "        df_plot = df\n",
    "\n",
    "    y_true = df_plot['true_label'].to_numpy(dtype=np.int8)\n",
    "    y_pred = df_plot['pred_label'].to_numpy(dtype=np.int8)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))\n",
    "    if normalize:\n",
    "        cm = cm.astype(float) / cm.sum(axis=1)[:, None]\n",
    "        fmt, title = '.2f', f'Normalized Confusion Matrix{title_suffix}'\n",
    "    else:\n",
    "        fmt, title = 'd', f'Confusion Matrix{title_suffix}'\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=True, fmt=fmt, cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(title)\n",
    "    plt.xticks(rotation=45); plt.yticks(rotation=0)\n",
    "    plt.tight_layout(); plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=3))\n",
    "\n",
    "def plot_classification_confidence_analysis(df: pd.DataFrame, sample_idx: Optional[np.ndarray] = None, title_suffix: str = \"\"):\n",
    "    if sample_idx is not None:\n",
    "        df_plot = df.iloc[sample_idx]\n",
    "    else:\n",
    "        df_plot = df\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    conf = df_plot['confidence'].to_numpy(dtype=np.float32)\n",
    "    is_correct = df_plot['is_correct'].to_numpy(dtype=np.bool_)\n",
    "    y_true = df_plot['true_label'].to_numpy(dtype=np.int8)\n",
    "\n",
    "    axes[0, 0].hist(conf[is_correct], bins=30, alpha=0.7, label='Correct')\n",
    "    axes[0, 0].hist(conf[~is_correct], bins=30, alpha=0.7, label='Incorrect')\n",
    "    axes[0, 0].set_xlabel('Confidence'); axes[0, 0].set_ylabel('Count'); axes[0, 0].set_title('Confidence Distribution: Correct vs Incorrect')\n",
    "    axes[0, 0].legend(); axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    for cls in range(len(class_names)):\n",
    "        cls_mask_correct = (y_true == cls) & is_correct\n",
    "        cls_mask_incorrect = (y_true == cls) & ~is_correct\n",
    "        if cls_mask_correct.any():\n",
    "            axes[0,1].hist(conf[cls_mask_correct], bins=20, alpha=0.6, color=class_colors[cls], label=class_names[cls])\n",
    "        if cls_mask_incorrect.any():\n",
    "            axes[1,0].hist(conf[cls_mask_incorrect], bins=20, alpha=0.6, color=class_colors[cls], label=class_names[cls])\n",
    "\n",
    "    axes[0,1].set_xlabel('Confidence'); axes[0,1].set_ylabel('Count'); axes[0,1].set_title('Confidence Distribution by Class (Correct Predictions)')\n",
    "    axes[0,1].legend(); axes[0,1].grid(alpha=0.3)\n",
    "    axes[1,0].set_xlabel('Confidence'); axes[1,0].set_ylabel('Count'); axes[1,0].set_title('Confidence Distribution by Class (Incorrect Predictions)')\n",
    "    axes[1,0].legend(); axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "    bins = np.linspace(0,1,11)\n",
    "    accuracy_by_bin = np.array([is_correct[(conf >= bins[i]) & (conf < bins[i+1])].mean() if np.any((conf >= bins[i]) & (conf < bins[i+1])) else 0 for i in range(len(bins)-1)])\n",
    "    axes[1,1].plot(bins[:-1]+0.05, accuracy_by_bin, 'o-', linewidth=2, markersize=8)\n",
    "    axes[1,1].set_xlabel('Confidence Bin'); axes[1,1].set_ylabel('Accuracy'); axes[1,1].set_title('Accuracy vs Confidence'); axes[1,1].grid(alpha=0.3)\n",
    "    axes[1,1].set_ylim(0,1)\n",
    "\n",
    "    plt.tight_layout(); plt.show(); plt.close()\n",
    "\n",
    "# The more heavy distribution plots can accept y_true/y_score or df and accept sample_idx to be efficient\n",
    "def plot_class_wise_distributions(df: pd.DataFrame, y_true: np.ndarray, y_score: np.ndarray,\n",
    "                                  tt_threshold: float, sample_idx: Optional[np.ndarray] = None, title_suffix: str = \"\"):\n",
    "    if sample_idx is not None:\n",
    "        y_true_plot = y_true[sample_idx]\n",
    "        y_score_plot = y_score[sample_idx, :]\n",
    "    else:\n",
    "        y_true_plot = y_true\n",
    "        y_score_plot = y_score\n",
    "\n",
    "    global_min = float(np.nanmin(y_score_plot))\n",
    "    global_max = float(np.nanmax(y_score_plot))\n",
    "    num_bins = 50\n",
    "    common_bins = np.linspace(global_min, global_max, num_bins + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    title_suffix_full = f\" ({', '.join([f'{class_names[i]}: {(y_true_plot==i).sum():,}' for i in range(len(class_names))])})\"\n",
    "    fig.suptitle(f\"Class-wise Score Distributions with True-True Threshold{title_suffix}{title_suffix_full}\", fontsize=14)\n",
    "\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        ax = axes[idx]\n",
    "        ax.set_title(class_name)\n",
    "        ax.set_xlabel('Score'); ax.set_ylabel('Density')\n",
    "        optimal_thresh = tt_threshold if class_name == 'True-True' else 0.5\n",
    "\n",
    "        for truth_type in sorted(np.unique(y_true_plot)):\n",
    "            scores = y_score_plot[y_true_plot == truth_type, idx]\n",
    "            fraction_above = np.mean(scores > optimal_thresh) if scores.size > 0 else 0.0\n",
    "            ax.hist(scores, bins=common_bins, density=True, alpha=0.6, label=f'Truth {truth_type} (> {fraction_above:.3%})', color=class_colors[truth_type])\n",
    "\n",
    "        ax.axvline(optimal_thresh, color='black', linestyle='dashed', linewidth=2, label=f'Thresh: {optimal_thresh:.4f}')\n",
    "        ax.legend()\n",
    "    if len(class_names) < len(axes):\n",
    "        axes[len(class_names)].set_visible(False)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95]); plt.show(); plt.close()\n",
    "\n",
    "def plot_truth_type_distributions(y_true: np.ndarray, y_score: np.ndarray,\n",
    "                                  tt_threshold: float, sample_idx: Optional[np.ndarray] = None, title_suffix: str = \"\"):\n",
    "    if sample_idx is not None:\n",
    "        y_true_plot = y_true[sample_idx]\n",
    "        y_score_plot = y_score[sample_idx, :]\n",
    "    else:\n",
    "        y_true_plot = y_true\n",
    "        y_score_plot = y_score\n",
    "\n",
    "    global_min = float(np.nanmin(y_score_plot))\n",
    "    global_max = float(np.nanmax(y_score_plot))\n",
    "    num_bins = 50\n",
    "    common_bins = np.linspace(global_min, global_max, num_bins + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    title_suffix_full = f\" ({', '.join([f'{class_names[i]}: {(y_true_plot==i).sum():,}' for i in range(len(class_names))])})\"\n",
    "    fig.suptitle(f\"Truth Type-wise Score Distributions for All Output Classes{title_suffix}{title_suffix_full}\", fontsize=14)\n",
    "\n",
    "    unique_truth_types = sorted(np.unique(y_true_plot))\n",
    "    for idx, truth_type in enumerate(unique_truth_types):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "        ax = axes[idx]\n",
    "        ax.set_title(f'Truth Type {truth_type} ({class_names[truth_type]})')\n",
    "        ax.set_xlabel('Score'); ax.set_ylabel('Density')\n",
    "        associated_class_idx = truth_type\n",
    "        optimal_threshold = tt_threshold if class_names[associated_class_idx] == 'True-True' else 0.5\n",
    "\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            scores = y_score_plot[y_true_plot == truth_type, class_idx]\n",
    "            fraction_above = np.mean(scores > optimal_threshold) * 100 if scores.size > 0 else 0.0\n",
    "            ax.hist(scores, bins=common_bins, density=True, alpha=0.6, label=f'{class_name} ({fraction_above:.1f}%)', color=class_colors[class_idx])\n",
    "\n",
    "        ax.axvline(optimal_threshold, color='black', linestyle='dashed', linewidth=2, label=f'Thresh: {optimal_threshold:.4f}')\n",
    "        ax.legend()\n",
    "    for idx in range(len(unique_truth_types), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95]); plt.show(); plt.close()\n",
    "\n",
    "def plot_loss_and_accuracy(metrics, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training/test loss and accuracy curves from your metrics dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    train_loss = np.array(metrics.get(\"train_loss\", []))\n",
    "    test_loss  = np.array(metrics.get(\"test_loss\", []))\n",
    "    train_acc  = np.array(metrics.get(\"train_acc\", []))\n",
    "    test_acc   = np.array(metrics.get(\"test_acc\", []))\n",
    "\n",
    "    epochs = np.arange(1, len(train_loss) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axs[0].plot(epochs, train_loss, label=\"Train Loss\", linewidth=2)\n",
    "    axs[0].plot(epochs, test_loss, label=\"Test Loss\", linewidth=2)\n",
    "    axs[0].set_title(\"Loss Over Epochs\", fontsize=14)\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axs[1].plot(epochs, train_acc, label=\"Train Accuracy\", linewidth=2)\n",
    "    axs[1].plot(epochs, test_acc, label=\"Test Accuracy\", linewidth=2)\n",
    "    axs[1].set_title(\"Accuracy Over Epochs\", fontsize=14)\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_ylim(0, 1.0)\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "        print(f\"Saved loss/accuracy curves to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def load_metrics(model_name: str, file_pwd: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Load training metrics from a file.\n",
    "    Adjust this based on how you save your metrics.\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    \n",
    "    # Try different file formats\n",
    "    possible_paths = [\n",
    "        f\"{file_pwd}/{model_name}_metrics.pkl\",\n",
    "        f\"{file_pwd}/{model_name}_metrics.json\",\n",
    "        f\"{file_pwd}/metrics_{model_name}.pkl\",\n",
    "        f\"{file_pwd}/metrics_{model_name}.json\",\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        try:\n",
    "            if path.endswith('.pkl'):\n",
    "                with open(path, 'rb') as f:\n",
    "                    metrics = pickle.load(f)\n",
    "                print(f\"Loaded metrics from {path}\")\n",
    "                return metrics\n",
    "            elif path.endswith('.json'):\n",
    "                with open(path, 'r') as f:\n",
    "                    metrics = json.load(f)\n",
    "                print(f\"Loaded metrics from {path}\")\n",
    "                return metrics\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"No metrics file found\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87749ee3-3dde-4cb6-9e9a-7a82c92db332",
   "metadata": {},
   "source": [
    "## Event Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158664de-2835-4908-9dc5-96b85a5449e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_features_comprehensive(df: pd.DataFrame, num_events: int = 5,\n",
    "                                      features: List[str] = ['snr', 'eta', 'phi'],\n",
    "                                      log: bool = False):\n",
    "    unique_events = df['event_id'].unique()\n",
    "    events_to_plot = unique_events[:min(num_events, len(unique_events))]\n",
    "    class_names_dict = {0: 'Lone-Lone', 1: 'True-True', 2: 'Cluster-Lone', 3: 'Lone-Cluster', 4: 'Cluster-Cluster'}\n",
    "    # Pre-group once\n",
    "    grouped = df.groupby('event_id', sort=False)\n",
    "    for event_id in events_to_plot:\n",
    "        if event_id not in grouped.groups:\n",
    "            continue\n",
    "        event_data = grouped.get_group(event_id)\n",
    "        n_edges = len(event_data)\n",
    "        if n_edges == 0:\n",
    "            continue\n",
    "        print(f\"\\nEvent {event_id}: {n_edges} edges\")\n",
    "        class_counts = event_data['true_label'].value_counts().to_dict()\n",
    "        for cls, count in sorted(class_counts.items()):\n",
    "            print(f\"  Class {cls} ({class_names_dict[cls]}): {count} edges\")\n",
    "        n_features = len(features)\n",
    "        fig, axes = plt.subplots(n_features, 2, figsize=(15, 5 * n_features))\n",
    "        if n_features == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        true_labels = event_data['true_label'].to_numpy()\n",
    "        class_masks = {cls: (true_labels == cls) for cls in range(5)}\n",
    "        for feature_idx, feature in enumerate(features):\n",
    "            s_vals = event_data[f'{feature}_source'].to_numpy(dtype=np.float32)\n",
    "            t_vals = event_data[f'{feature}_target'].to_numpy(dtype=np.float32)\n",
    "            all_vals = np.concatenate((s_vals, t_vals), dtype=np.float32)\n",
    "            use_log = log and feature.lower() == 'snr'\n",
    "            x_label = f\"Log {feature.upper()}\" if use_log else feature.upper()\n",
    "            ax_left = axes[feature_idx, 0]\n",
    "            ax_left.hist(all_vals, bins=100, alpha=0.7, log=use_log)\n",
    "            ax_left.set_title(f'Event {event_id}: Overall {x_label} Distribution\\n({len(all_vals)} values, range: {np.nanmin(all_vals):.2f}â€“{np.nanmax(all_vals):.2f})')\n",
    "            ax_left.set_xlabel(x_label); ax_left.set_ylabel('Count'); ax_left.grid(True, alpha=0.3)\n",
    "            ax_right = axes[feature_idx, 1]\n",
    "            for cls in range(5):\n",
    "                mask = class_masks[cls]\n",
    "                if not np.any(mask):\n",
    "                    continue\n",
    "                cls_vals = np.concatenate((s_vals[mask], t_vals[mask]), dtype=np.float32)\n",
    "                ax_right.hist(cls_vals, bins=100, alpha=0.5, color=class_colors[cls], label=f'Class {cls} ({class_names_dict[cls]})', log=use_log)\n",
    "            ax_right.set_title(f'Event {event_id}: {x_label} by True Class'); ax_right.set_xlabel(x_label); ax_right.set_ylabel('Count'); ax_right.legend(); ax_right.grid(True, alpha=0.3)\n",
    "        plt.tight_layout(); plt.show(); plt.close()\n",
    "        # Summary stats\n",
    "        print(f\"\\nEvent {event_id} Feature Statistics:\")\n",
    "        for feature in features:\n",
    "            s = event_data[f'{feature}_source'].to_numpy(dtype=np.float32)\n",
    "            t = event_data[f'{feature}_target'].to_numpy(dtype=np.float32)\n",
    "            corr = np.corrcoef(s, t)[0, 1] if len(s) > 1 else np.nan\n",
    "            print(f\"  {feature.upper()}: Source mean={s.mean():.3f}, Target mean={t.mean():.3f}, Correlation={corr:.3f}\")\n",
    "\n",
    "def plot_error_analysis_by_features(df: pd.DataFrame, title_suffix: str = \"\"):\n",
    "    errors = df[df['is_correct'] == False]\n",
    "    if len(errors) == 0:\n",
    "        print(\"No errors found for analysis\"); return\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,12))\n",
    "    spatial = df['spatial_distance'].to_numpy(dtype=np.float32)\n",
    "    avg_snr = df['avg_snr'].to_numpy(dtype=np.float32)\n",
    "    is_correct = df['is_correct'].to_numpy(dtype=np.bool_)\n",
    "    # Error rate vs spatial\n",
    "    bins = np.linspace(0, np.nanmax(spatial), 10)\n",
    "    error_rates = [np.mean(~is_correct[(spatial >= bins[i]) & (spatial < bins[i+1])]) for i in range(len(bins)-1)]\n",
    "    axes[0,0].plot(bins[:-1]+np.diff(bins)[0]/2, error_rates, 'o-', linewidth=2)\n",
    "    axes[0,0].set_xlabel('Spatial Distance'); axes[0,0].set_ylabel('Error Rate'); axes[0,0].set_title('Error Rate vs Spatial Distance'); axes[0,0].grid(alpha=0.3)\n",
    "    # Error rate vs avg SNR\n",
    "    bins = np.linspace(0, np.nanmax(avg_snr), 10)\n",
    "    error_rates_snr = [np.mean(~is_correct[(avg_snr >= bins[i]) & (avg_snr < bins[i+1])]) for i in range(len(bins)-1)]\n",
    "    axes[0,1].plot(bins[:-1]+np.diff(bins)[0]/2, error_rates_snr, 'o-', linewidth=2, color='orange')\n",
    "    axes[0,1].set_xlabel('Average SNR'); axes[0,1].set_ylabel('Error Rate'); axes[0,1].set_title('Error Rate vs Average SNR'); axes[0,1].grid(alpha=0.3)\n",
    "    # Error type heatmap\n",
    "    error_types = errors.groupby(['true_label','pred_label']).size().unstack(fill_value=0)\n",
    "    im = axes[1,0].imshow(error_types.values, cmap='Reds', aspect='auto')\n",
    "    axes[1,0].set_xlabel('Predicted Label'); axes[1,0].set_ylabel('True Label'); axes[1,0].set_title('Error Type Heatmap')\n",
    "    axes[1,0].set_xticks(range(len(class_names))); axes[1,0].set_xticklabels(class_names, rotation=45)\n",
    "    axes[1,0].set_yticks(range(len(class_names))); axes[1,0].set_yticklabels(class_names)\n",
    "    plt.colorbar(im, ax=axes[1,0])\n",
    "    # confidence by error type\n",
    "    for true_label in np.unique(errors['true_label']):\n",
    "        err_mask = (errors['true_label'] == true_label)\n",
    "        for pred_label in np.unique(errors['pred_label'][err_mask]):\n",
    "            conf_vals = errors.loc[err_mask & (errors['pred_label'] == pred_label), 'confidence'].to_numpy()\n",
    "            if conf_vals.size > 0:\n",
    "                axes[1,1].hist(conf_vals, bins=20, alpha=0.6, label=f'{class_names[int(true_label)]}â†’{class_names[int(pred_label)]}')\n",
    "    axes[1,1].set_xlabel('Confidence'); axes[1,1].set_ylabel('Count'); axes[1,1].set_title('Confidence Distribution by Error Type'); axes[1,1].legend(); axes[1,1].grid(alpha=0.3)\n",
    "    plt.tight_layout(); plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183672ae-728f-4a7c-94f5-6a98bdcb79da",
   "metadata": {},
   "source": [
    "## Multi-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa3a61-5aa4-4c56-aa43-c6fb573fe655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_models(model_names=[\"1_layer\", \"2_layer\", \"3_layer\", \"6_layer\", \"9_layer\", \"12_layer\"]):\n",
    "    \"\"\"Compare performance across multiple models\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            df = load_comprehensive_data(model_name, file_pwd)\n",
    "            \n",
    "            # Basic metrics\n",
    "            accuracy = df['is_correct'].mean()\n",
    "            total_samples = len(df)\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            class_accuracies = []\n",
    "            for cls in range(5):\n",
    "                cls_mask = df['true_label'] == cls\n",
    "                if cls_mask.any():\n",
    "                    cls_acc = df[cls_mask]['is_correct'].mean()\n",
    "                    class_accuracies.append(cls_acc)\n",
    "                else:\n",
    "                    class_accuracies.append(0)\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'model': model_name,\n",
    "                'layers': int(model_name.split('_')[0]),\n",
    "                'accuracy': accuracy,\n",
    "                'total_samples': total_samples,\n",
    "                'class_0_acc': class_accuracies[0],\n",
    "                'class_1_acc': class_accuracies[1],\n",
    "                'class_2_acc': class_accuracies[2],\n",
    "                'class_3_acc': class_accuracies[3],\n",
    "                'class_4_acc': class_accuracies[4]\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {model_name}: accuracy = {accuracy:.3%}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found for {model_name}, skipping...\")\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "def plot_model_comparison(comparison_df):\n",
    "    \"\"\"Plot comparison across different models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Overall accuracy by number of layers\n",
    "    axes[0, 0].plot(comparison_df['layers'], comparison_df['accuracy'], 'o-', linewidth=2, markersize=8)\n",
    "    axes[0, 0].set_xlabel('Number of Layers')\n",
    "    axes[0, 0].set_ylabel('Overall Accuracy')\n",
    "    axes[0, 0].set_title('Model Accuracy vs Number of Layers')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Per-class accuracy comparison\n",
    "    class_acc_cols = ['class_0_acc', 'class_1_acc', 'class_2_acc', 'class_3_acc', 'class_4_acc']\n",
    "    x_pos = np.arange(len(comparison_df))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, (cls_name, color) in enumerate(zip(class_names, class_colors)):\n",
    "        axes[0, 1].bar(x_pos + i*width, comparison_df[f'class_{i}_acc'], \n",
    "                       width, label=cls_name, color=color, alpha=0.7)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Model')\n",
    "    axes[0, 1].set_ylabel('Class Accuracy')\n",
    "    axes[0, 1].set_title('Per-Class Accuracy by Model')\n",
    "    axes[0, 1].set_xticks(x_pos + 2*width)\n",
    "    axes[0, 1].set_xticklabels(comparison_df['model'], rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confidence distributions across models\n",
    "    for model_name in comparison_df['model']:\n",
    "        try:\n",
    "            df = load_comprehensive_data(model_name, file_pwd)\n",
    "            axes[1, 0].hist(df['confidence'], bins=30, alpha=0.6, \n",
    "                           label=f\"{model_name}\", density=True)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Confidence')\n",
    "    axes[1, 0].set_ylabel('Density')\n",
    "    axes[1, 0].set_title('Confidence Distribution by Model')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error rate by spatial distance across models\n",
    "    for model_name in comparison_df['model']:\n",
    "        try:\n",
    "            df = load_comprehensive_data(model_name, file_pwd)\n",
    "            spatial_bins = np.linspace(0, df['spatial_distance'].max(), 10)\n",
    "            error_rates = []\n",
    "            for i in range(len(spatial_bins) - 1):\n",
    "                bin_mask = (df['spatial_distance'] >= spatial_bins[i]) & (df['spatial_distance'] < spatial_bins[i + 1])\n",
    "                bin_data = df[bin_mask]\n",
    "                if len(bin_data) > 0:\n",
    "                    error_rate = (bin_data['is_correct'] == False).mean()\n",
    "                    error_rates.append(error_rate)\n",
    "                else:\n",
    "                    error_rates.append(0)\n",
    "            \n",
    "            axes[1, 1].plot(spatial_bins[:-1] + np.diff(spatial_bins)[0]/2, \n",
    "                           error_rates, 'o-', label=model_name, alpha=0.7)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Spatial Distance')\n",
    "    axes[1, 1].set_ylabel('Error Rate')\n",
    "    axes[1, 1].set_title('Error Rate vs Spatial Distance by Model')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd93b812-941a-4c03-979b-574ef3fd469e",
   "metadata": {},
   "source": [
    "## Main Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc2907-eea3-4b6c-8abe-0480ca7657f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_model_analysis(df: pd.DataFrame, model_name: str = \"\",\n",
    "                                     fast_mode: bool = True, max_plot_rows: int = 2_000_000,\n",
    "                                     metrics: Optional[Dict] = None):  \n",
    "    \"\"\"\n",
    "    Main analysis runner. If fast_mode=True:\n",
    "      - skip heavy plotting or subsample for plotting\n",
    "    \"\"\"\n",
    "    title_suffix = f\" - {model_name}\" if model_name else \"\"\n",
    "    print(f\"=== COMPREHENSIVE MODEL ANALYSIS{title_suffix} ===\")\n",
    "\n",
    "    # Downcast dtypes for memory & speed\n",
    "    df = downcast_dtypes(df)\n",
    "\n",
    "    # Prepare roc arrays once (cached)\n",
    "    y_true, y_score = prepare_roc_data_from_df(df)\n",
    "\n",
    "    # Apply True-True only threshold (vectorized)\n",
    "    df_optimized, tt_threshold = apply_tt_only_threshold_fast(df, y_true, y_score, tt_tpr_target=0.99)\n",
    "    print(f\"âœ… True-True threshold for 99% TPR: {tt_threshold:.4f}\")\n",
    "\n",
    "    print(f\"Total samples: {len(df_optimized):,}\")\n",
    "    print(f\"Overall accuracy: {df_optimized['is_correct'].mean():.3%}\")\n",
    "\n",
    "    print(\"Class distribution:\")\n",
    "    for cls_idx, cname in enumerate(class_names):\n",
    "        count = int((df_optimized['true_label'] == cls_idx).sum())\n",
    "        acc = df_optimized[df_optimized['true_label'] == cls_idx]['is_correct'].mean() if count else 0\n",
    "        true_pos = int(((df_optimized['true_label'] == cls_idx) & (df_optimized['pred_label'] == cls_idx)).sum())\n",
    "        recall = true_pos / count if count > 0 else 0\n",
    "        print(f\"  {cname}: {count:,} samples, accuracy: {acc:.3%}, recall: {recall:.3%}\")\n",
    "\n",
    "    tt_true_count = int((df_optimized['true_label'] == class_names.index(\"True-True\")).sum())\n",
    "    tt_recall = ((df_optimized['true_label'] == class_names.index(\"True-True\")) & (df_optimized['pred_label'] == class_names.index(\"True-True\"))).sum() / tt_true_count\n",
    "    print(f\"\\nâœ… Verified True-True Recall: {tt_recall:.3%}\")\n",
    "\n",
    "    # ADD THIS SECTION - Plot training curves (always show this - it's lightweight)\n",
    "    if metrics is not None:\n",
    "        print(\"\\nðŸ“Š Plotting training history...\")\n",
    "        plot_loss_and_accuracy(metrics)\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No metrics provided - skipping training history plots\")\n",
    "\n",
    "    # Decide sampling for plotting\n",
    "    n = len(df_optimized)\n",
    "    sample_idx = downsample_indices(n, max_rows=max_plot_rows)\n",
    "    if fast_mode:\n",
    "        print(\"âš¡ Fast mode enabled: plotting will use subsampling or be skipped for heavy plots.\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ Full plotting mode: generating all plots (this may take time).\")\n",
    "\n",
    "    # Plotting: pass y_true, y_score to avoid repeated prepare calls\n",
    "    if not fast_mode:\n",
    "        # full resolution (but still safe-guard with sampling for extremely large n)\n",
    "        # REMOVED plot_loss_and_accuracy from here since we moved it above\n",
    "        plot_roc_curves(y_true, y_score, sample_idx=None if sample_idx is None else sample_idx, title_suffix=title_suffix)\n",
    "        plot_precision_recall_curves(y_true, y_score, sample_idx=None if sample_idx is None else sample_idx, title_suffix=title_suffix)\n",
    "        plot_confusion_matrix_from_df(df_optimized, title_suffix=title_suffix, normalize=True, sample_idx=None if sample_idx is None else sample_idx)\n",
    "        plot_confusion_matrix_from_df(df_optimized, title_suffix=title_suffix, normalize=False, sample_idx=None if sample_idx is None else sample_idx)\n",
    "        plot_classification_confidence_analysis(df_optimized, sample_idx=None if sample_idx is None else sample_idx, title_suffix=title_suffix)\n",
    "        plot_error_analysis_by_features(df_optimized, title_suffix=title_suffix)\n",
    "        plot_class_wise_distributions(df_optimized, y_true, y_score, tt_threshold, sample_idx=None if sample_idx is None else sample_idx, title_suffix=title_suffix)\n",
    "        plot_truth_type_distributions(y_true, y_score, tt_threshold, sample_idx=None if sample_idx is None else sample_idx, title_suffix=title_suffix)\n",
    "        plot_event_features_comprehensive(df_optimized, num_events=5)\n",
    "    else:\n",
    "        # fast mode: subsampled plots + skip some heavy ones\n",
    "        print(\"Generating key diagnostic plots (subsampled)...\")\n",
    "        # REMOVED plot_loss_and_accuracy from here since we moved it above\n",
    "        plot_roc_curves(y_true, y_score, sample_idx=sample_idx, title_suffix=title_suffix + \" (subsampled)\")\n",
    "        plot_precision_recall_curves(y_true, y_score, sample_idx=sample_idx, title_suffix=title_suffix + \" (subsampled)\")\n",
    "        plot_confusion_matrix_from_df(df_optimized, title_suffix=title_suffix + \" (subsampled)\", normalize=True, sample_idx=sample_idx)\n",
    "        plot_classification_confidence_analysis(df_optimized, sample_idx=sample_idx, title_suffix=title_suffix)\n",
    "        # heavy distribution plots (subsampled)\n",
    "        plot_class_wise_distributions(df_optimized, y_true, y_score, tt_threshold, sample_idx=sample_idx, title_suffix=title_suffix + \" (subsampled)\")\n",
    "        plot_truth_type_distributions(y_true, y_score, tt_threshold, sample_idx=sample_idx, title_suffix=title_suffix + \" (subsampled)\")\n",
    "        # skip per-event comprehensive plots in fast mode (they can be enabled when needed)\n",
    "        print(\"Note: event-level comprehensive plots skipped in fast mode to save time. Re-run with fast_mode=False to generate them.\")\n",
    "\n",
    "    return df_optimized, tt_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6818175-d43a-4b0b-be12-a89c9325dec7",
   "metadata": {},
   "source": [
    "## Cluster Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1602f-0373-4995-b2a3-e80e89dba85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_snr_distributions(df):\n",
    "    \"\"\"Create 3D SNR distribution plots\"\"\"\n",
    "    # Your existing code - works perfectly!\n",
    "    eta  = df['eta_source']\n",
    "    phi  = df['phi_source']\n",
    "    snr  = df['snr_source']\n",
    "\n",
    "    n_eta, n_phi = 30, 30\n",
    "    hist_snr, eta_edges, phi_edges = np.histogram2d(\n",
    "        eta, phi, bins=[n_eta, n_phi], weights=snr\n",
    "    )\n",
    "    counts, _, _ = np.histogram2d(eta, phi, bins=[n_eta, n_phi])\n",
    "    mean_snr = np.divide(hist_snr, counts, out=np.zeros_like(hist_snr), where=counts>0)\n",
    "\n",
    "    eta_centers = 0.5*(eta_edges[:-1] + eta_edges[1:])\n",
    "    phi_centers = 0.5*(phi_edges[:-1] + phi_edges[1:])\n",
    "    eta_grid, phi_grid = np.meshgrid(eta_centers, phi_centers, indexing='ij')\n",
    "\n",
    "    # Plot 1: Total SNR Distribution\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    dx = dy = (eta_edges[1]-eta_edges[0]) * 0.8\n",
    "    dz = hist_snr.ravel()\n",
    "\n",
    "    # Normalize dz for color mapping\n",
    "    norm = plt.Normalize(dz.min(), dz.max())\n",
    "    colors = cm.viridis(norm(dz))\n",
    "\n",
    "    ax.bar3d(\n",
    "        eta_grid.ravel(),\n",
    "        phi_grid.ravel(),\n",
    "        np.zeros_like(dz),\n",
    "        dx, dy, dz,\n",
    "        color=colors,\n",
    "        shade=True,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('Î·')\n",
    "    ax.set_ylabel('Ï†')\n",
    "    ax.set_zlabel('Total SNR')\n",
    "    ax.set_title('3D SNR Distribution - Source Nodes')\n",
    "\n",
    "    mappable = cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "    fig.colorbar(mappable, ax=ax, shrink=0.6, label='SNR')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Mean SNR Distribution\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    dz = mean_snr.ravel()\n",
    "\n",
    "    # Normalize dz for color mapping\n",
    "    norm = plt.Normalize(dz.min(), dz.max())\n",
    "    colors = cm.viridis(norm(dz))\n",
    "\n",
    "    ax.bar3d(\n",
    "        eta_grid.ravel(),\n",
    "        phi_grid.ravel(),\n",
    "        np.zeros_like(dz),\n",
    "        dx, dy, dz,\n",
    "        color=colors,\n",
    "        shade=True,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('Î·')\n",
    "    ax.set_ylabel('Ï†')\n",
    "    ax.set_zlabel('Mean SNR')\n",
    "    ax.set_title('3D Mean SNR Distribution - Source Nodes')\n",
    "\n",
    "    mappable = cm.ScalarMappable(norm=norm, cmap='viridis')\n",
    "    fig.colorbar(mappable, ax=ax, shrink=0.6, label='Mean SNR')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_2d_class_distributions(df):\n",
    "    \"\"\"Create 2D scatter plots by class\"\"\"\n",
    "    # Select only classes 1â€“4 (or use 0-4 for all classes)\n",
    "    mask = df['true_label'].isin([1,2,3,4])\n",
    "    subset = df[mask]\n",
    "\n",
    "    # Custom color palette matching your class definitions\n",
    "    class_colors_dict = {\n",
    "        1: 'orange',  # True-True\n",
    "        2: 'green',   # Cluster-Lone  \n",
    "        3: 'red',     # Lone-Cluster\n",
    "        4: 'purple'   # Cluster-Cluster\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sc = plt.scatter(\n",
    "        subset['eta_source'],\n",
    "        subset['phi_source'],\n",
    "        c=subset['true_label'].map(class_colors_dict),\n",
    "        s=15,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Create custom legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=8, label='True-True'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=8, label='Cluster-Lone'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=8, label='Lone-Cluster'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='purple', markersize=8, label='Cluster-Cluster')\n",
    "    ]\n",
    "\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    plt.xlabel('Î· (Eta)')\n",
    "    plt.ylabel('Ï† (Phi)')\n",
    "    plt.title('Etaâ€“Phi Distribution by True Class (Source Nodes)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_comparison_visualizations(df):\n",
    "    \"\"\"Create comparison visualizations\"\"\"\n",
    "    # Source vs Target comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Source nodes\n",
    "    sc1 = ax1.scatter(df['eta_source'], df['phi_source'], \n",
    "                     c=df['snr_source'], cmap='viridis', s=10, alpha=0.6)\n",
    "    ax1.set_xlabel('Î·')\n",
    "    ax1.set_ylabel('Ï†')\n",
    "    ax1.set_title('Source Nodes - Colored by SNR')\n",
    "    plt.colorbar(sc1, ax=ax1, label='SNR')\n",
    "\n",
    "    # Target nodes  \n",
    "    sc2 = ax2.scatter(df['eta_target'], df['phi_target'],\n",
    "                     c=df['snr_target'], cmap='viridis', s=10, alpha=0.6)\n",
    "    ax2.set_xlabel('Î·')\n",
    "    ax2.set_ylabel('Ï†')\n",
    "    ax2.set_title('Target Nodes - Colored by SNR')\n",
    "    plt.colorbar(sc2, ax=ax2, label='SNR')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Correct vs Incorrect predictions\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    correct = df[df['is_correct'] == True]\n",
    "    incorrect = df[df['is_correct'] == False]\n",
    "\n",
    "    plt.scatter(correct['eta_source'], correct['phi_source'], \n",
    "               c='green', s=10, alpha=0.6, label='Correct Predictions')\n",
    "    plt.scatter(incorrect['eta_source'], incorrect['phi_source'], \n",
    "               c='red', s=10, alpha=0.6, label='Incorrect Predictions')\n",
    "\n",
    "    plt.xlabel('Î· (Eta)')\n",
    "    plt.ylabel('Ï† (Phi)')\n",
    "    plt.title('Prediction Accuracy in Eta-Phi Space')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2723511-e029-4625-9aa8-03e3cf8b2b77",
   "metadata": {},
   "source": [
    "## Execution: Single Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9362e-8185-43c1-bf79-7147c0be8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # === USER CONFIGURE HERE ===\n",
    "    model_name = \"fixed_generator_bs1\"   # change as needed\n",
    "    file_pwd = \"/storage/mxg1065/fixed_batch_size_models\"\n",
    "    use_polars = True            # set True if you want Polars (and have it installed)\n",
    "    fast_mode = True              # True = fast (subsampled plots, skip heavy event-level plots)\n",
    "    max_plot_rows = 2_000_000     # cap for plotting; adjust if you have more memory\n",
    "    # ============================\n",
    "\n",
    "    print(f\"ðŸš€ Starting analysis for {model_name} model...\")\n",
    "    \n",
    "    metrics = load_metrics(model_name, file_pwd)\n",
    "    \n",
    "    df = load_comprehensive_data(model_name, file_pwd, columns=DEFAULT_COLUMNS, use_polars=use_polars)\n",
    "\n",
    "    print(\"ðŸ“ˆ Basic Statistics (pre-optimization):\")\n",
    "    print(f\"Total samples: {len(df):,}\")\n",
    "    if 'is_correct' in df.columns:\n",
    "        print(f\"Overall accuracy: {df['is_correct'].mean():.3%}\")\n",
    "    print(\"\\nClass Distribution (pre-optimization):\")\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        if cls_idx in df['true_label'].unique():\n",
    "            count = int((df['true_label'] == cls_idx).sum())\n",
    "            acc = df[df['true_label'] == cls_idx]['is_correct'].mean() if count > 0 else 0\n",
    "            print(f\"  {cls_name}: {count:,} samples, accuracy: {acc:.3%}\")\n",
    "        else:\n",
    "            print(f\"  {cls_name}: 0 samples\")\n",
    "\n",
    "    # Run the fast/optimized analysis\n",
    "    df_optimized, tt_threshold = run_comprehensive_model_analysis(df, model_name=model_name, \n",
    "                                                                  fast_mode=fast_mode, \n",
    "                                                                  max_plot_rows=max_plot_rows,\n",
    "                                                                  metrics=metrics) \n",
    "\n",
    "    print(\"\\nAnalysis complete.\")\n",
    "    print(f\"True-True threshold: {tt_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f5d31-e19c-4e18-b96e-c61b954da98f",
   "metadata": {},
   "source": [
    "## Execution: Cluster Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdbe54-07a1-45fd-b1a2-394acf66191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŒŒ Creating cluster visualizations...\")\n",
    "plot_3d_snr_distributions(df)\n",
    "plot_2d_class_distributions(df)\n",
    "plot_comparison_visualizations(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03ca8e-0fdb-4614-b22a-89603f099c3f",
   "metadata": {},
   "source": [
    "## Execution: Multi-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb71f2-cee1-46e4-8f35-4c9a89304fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"ðŸ”„ Comparing multiple models...\")\n",
    "# comparison_df = compare_multiple_models([\"1_layer\", \"2_layer\", \"3_layer\", \"6_layer\", \"9_layer\", \"12_layer\"])\n",
    "# plot_model_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c80e9d-a4d9-4f79-be89-f5e059bd295c",
   "metadata": {},
   "source": [
    "## Event-Level Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c819cc-1f0e-4e96-aa75-2b471af939f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Analyzing specific events...\")\n",
    "interesting_events = df['event_id'].value_counts().head(5).index\n",
    "for event_id in interesting_events[:2]:\n",
    "    event_data = df[df['event_id'] == event_id]\n",
    "    print(f\"\\nðŸ” Event {event_id} Deep Dive:\")\n",
    "    print(f\"   Total edges: {len(event_data)}\")\n",
    "    print(f\"   Accuracy: {event_data['is_correct'].mean():.3%}\")\n",
    "    \n",
    "    # Plot feature distributions for this event\n",
    "    plot_event_features_comprehensive(df[df['event_id'] == event_id], \n",
    "                                    num_events=1, \n",
    "                                    features=['snr', 'eta', 'phi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6eb87-96ef-42f3-82b8-99d35574f491",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fc8c7-30b7-4679-9b17-90bf4f48c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Analysis completed successfully!\")\n",
    "print(\"\\nðŸ“Š Generated Analysis:\")\n",
    "print(\"âœ… ROC Curves & Precision-Recall Curves\")\n",
    "print(\"âœ… Confusion Matrices & Classification Reports\") \n",
    "print(\"âœ… Confidence Analysis & Error Analysis\")\n",
    "print(\"âœ… 3D SNR Distributions & Cluster Visualizations\")\n",
    "print(\"âœ… Multi-Model Comparisons\")\n",
    "print(\"âœ… Event-Level Deep Dives\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
